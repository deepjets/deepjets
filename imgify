#!/usr/bin/env python


def get_suffix(args):
    suffix = ''
    if args.suffix:
        suffix += '_{0}'.format(args.suffix)
    if args.zoom:
        suffix += '_zoomed'
    suffix += '_images'
    return suffix


def imgify(filename, args, multi=False, location=None):
    import os
    import sys
    import numpy as np
    from deepjets.preprocessing import preprocess, pixel_edges
    from deepjets.utils import jet_mass
    from deepjets.multi_progress import Writer
    from progressbar import Bar, ETA, Percentage, FormatLabel, ProgressBar
    import h5py

    pixel_size = args.pixel_size
    image_size = args.image_size
    zoom = args.zoom

    suffix = get_suffix(args)
    output = os.path.splitext(filename)[0] + suffix + '.h5'

    # check if output already exists and is finished
    if os.path.exists(output):
        try:
            h5py.File(output, 'r')
        except:
            print "{0} exists but is not complete. Will overwrite it.".format(output)
            pass
        else:
            print "{0} exists and is complete. Nothing to do.".format(output)
            return

    # create the output file
    h5_output = h5py.File(output, 'w')

    # read from the events file
    h5_input = h5py.File(filename, 'r')
    dset_jet = h5_input['jet']
    dset_trimmed_jet = h5_input['trimmed_jet']
    dset_subjets = h5_input['subjets']
    dset_trimmed_constit = h5_input['trimmed_constituents']
    dset_shrinkage = h5_input['shrinkage']
    dset_dr_subjets = h5_input['subjet_dr']
    dset_tau_1 = h5_input['tau_1']
    dset_tau_2 = h5_input['tau_2']
    dset_tau_3 = h5_input['tau_3']

    jet_size = h5_input['jet_size'][0]
    
    eta, phi = pixel_size.split(',')
    eta_size = float(eta) 
    phi_size = float(phi)

    edges = pixel_edges(
        jet_size=jet_size,
        pixel_size=(eta_size, phi_size),
        border_size=1 if zoom else 0)
    eta_edges, phi_edges = edges
    
    dset_images = h5_output.create_dataset(
        'images', (len(dset_subjets), image_size, image_size), dtype=np.double)

    auxvars_arrays = [
        dset_jet['pT'], dset_trimmed_jet['pT'],
        dset_jet['mass'], dset_trimmed_jet['mass'],
        dset_dr_subjets[:], dset_tau_1[:], dset_tau_2[:], dset_tau_3[:]]
    auxvars_names = 'pt,pt_trimmed,mass,mass_trimmed,subjet_dr,tau_1,tau_2,tau_3'

    if 'weights' in h5_input:
        auxvars_arrays.append(h5_input['weights'][:])
        auxvars_names += ',weights'
    
    auxvars = np.core.records.fromarrays(auxvars_arrays, names=auxvars_names)
    dset_auxvars = h5_output.create_dataset('auxvars', auxvars.shape, auxvars.dtype)
    dset_auxvars[:] = auxvars

    dset_eta_edges = h5_output.create_dataset(
        'eta_edges', (1, 2), dtype=np.double)
    dset_phi_edges = h5_output.create_dataset(
        'phi_edges', (1, 2), dtype=np.double)

    dset_eta_edges[0] = (eta_edges[0], eta_edges[-1])
    dset_phi_edges[0] = (phi_edges[0], phi_edges[-1])

    widgets = [FormatLabel(output), ' ', Percentage(), ' ', Bar('>'), ' ', ETA()]
    writer = Writer(location) if multi else sys.stderr
    pbar = ProgressBar(fd=writer, widgets=widgets, max_value=len(dset_subjets)).start()
    for ievent in xrange(len(dset_subjets)):
        subjets = dset_subjets[ievent]
        trimmed_constit = dset_trimmed_constit[ievent]
        if zoom:
            # 2 * mW / pT
            actual_size = 2 * 80.385 / dset_jet[ievent]['pT'];
            zoom_factor = max(jet_size / actual_size, 1.)
        else:
            zoom_factor = 1.
        image = preprocess(subjets, trimmed_constit, edges,
                           zoom=zoom_factor,
                           normalize=True,
                           out_width=image_size)
        # save the jet mass, pT and image
        dset_images[ievent] = image
        pbar.update(ievent)
    pbar.finish()
    h5_output.close()


if __name__ == '__main__':
    from argparse import ArgumentParser

    parser = ArgumentParser()
    parser.add_argument('--jobs', '-n', type=int, default=-1)
    parser.add_argument('--pixel-size', default='0.1,0.1')
    parser.add_argument('--image-size', type=int, default=25)
    parser.add_argument('--zoom', default=False, action='store_true')
    parser.add_argument('--suffix', default=None)
    parser.add_argument('--batch', default=None)
    parser.add_argument('--dry', action='store_true', default=False)
    parser.add_argument('files', nargs='+')
    args = parser.parse_args()
    
    from deepjets.parallel import map_pool, FuncWorker
    from deepjets.multi_progress import term
    import os
    import sys
    
    if args.batch is not None:
        # call me again but in a batch job for each input file
        import subprocess
        from deepjets.path_utils import mkdir_p

        cmd = sys.argv[:]
        # remove batch option
        idx = cmd.index('--batch')
        cmd.pop(idx)
        cmd.pop(idx)
        # remove filenames
        for filename in args.files:
            cmd.remove(filename)
        output_dir = os.getcwd()
        setup_cmd = "source {0}/setup.sh; cd {1};".format(
            os.path.dirname(os.path.realpath(__file__)),
            output_dir)
        log_path = os.path.join(output_dir, 'log')
        if args.dry:
            print "mkdir -p {0}".format(log_path)
        else:
            mkdir_p(log_path)
        # call self in batch job once per file
        for filename in args.files:
            name = os.path.splitext(os.path.basename(filename))[0] + get_suffix(args)
            cmd_file = ' '.join(cmd + [filename])
            cmd_batch = (
                'echo "{setup} {cmd_file}" | '
                'qsub -e {output_dir}/log -o {output_dir}/log '
                '-N {name} -l nodes=1:ppn=1 -q {queue};').format(
                    setup=setup_cmd,
                    cmd_file=cmd_file,
                    output_dir=output_dir,
                    queue=args.batch,
                    name=name)
            print cmd_batch
            if not args.dry:
                subprocess.call(cmd_batch, shell=True)
        sys.exit(0)

    if len(args.files) == 1:
        imgify(args.files[0], args)
    else:
        from contextlib import contextmanager
        
        @contextmanager                                                                 
        def do_nothing(*args, **kwargs):                                                
            yield 

        multi = len(args.files) > 1 and args.jobs != 1 and sys.stdout.isatty() 
        context = term.fullscreen if multi else do_nothing

        with context():
            map_pool(
                FuncWorker,
                [(imgify, filename, args, multi, (0, i))
                    for i, filename in enumerate(args.files)],
                    n_jobs=args.jobs)
