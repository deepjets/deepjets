#!/usr/bin/env python
"""
Perform jet clustering on HepMC or HDF5 input
"""

def cluster_file(filename, args, multi, location):
    import sys
    from deepjets.samples import create_jets_datasets
    from deepjets.generate import get_generator_input
    from deepjets.clustering import cluster
    from deepjets.detector import reconstruct
    from fnmatch import fnmatch
    from deepjets.multi_progress import Writer
    from progressbar import Bar, ETA, Percentage, FormatLabel, ProgressBar
    import numpy as np
    import h5py
    
    suffix = '_j{0:.1f}_sj{1:.2f}'.format(
        args.jet_size, args.subjet_size_fraction * args.jet_size).replace('.', 'p')
    if args.shrink:
        suffix += '_shrink'
    if args.delphes:
        suffix += '_delphes'
    suffix += '_jets'

    output = os.path.splitext(filename)[0] + suffix + '.h5'
    h5output = h5py.File(output, 'w')

    hepmc = fnmatch(os.path.splitext(filename)[1], '.hepmc*')
    if not hepmc:
        h5input = h5py.File(filename, 'r')

    if hepmc:
        cluster_input = get_generator_input('hepmc', filename)
        # assume we could be up to 20% wrong here
        num_events = long(cluster_input.estimate_num_events() * 1.2)
    else:
        cluster_input = h5input['events'] 
        num_events = len(cluster_input)

    if args.delphes:
        cluster_input = reconstruct(cluster_input)
    
    widgets = [FormatLabel(output), ' ', Percentage(), ' ', Bar('>'), ' ', ETA()]
    writer = Writer(location) if multi else sys.stderr
    pbar = ProgressBar(fd=writer, widgets=widgets, max_value=num_events).start()

    create_jets_datasets(h5output, num_events,
                         args.jet_size, args.subjet_size_fraction)

    dset_jet = h5output['jet']
    dset_trimmed_jet = h5output['trimmed_jet']
    dset_subjets = h5output['subjets']
    dset_constit = h5output['constituents']
    dset_trimmed_constit = h5output['trimmed_constituents']
    dset_shrinkage = h5output['shrinkage']
    dset_dr_subjets = h5output['subjet_dr']
    dset_tau_1 = h5output['tau_1']
    dset_tau_2 = h5output['tau_2']
    dset_tau_3 = h5output['tau_3']

    clustering_generator = cluster(
        cluster_input,
        jet_size=args.jet_size,
        subjet_size_fraction=args.subjet_size_fraction,
        subjet_pt_min_fraction=args.subjet_pt_min_fraction,
        subjet_dr_min=args.subjet_dr_min,
        trimmed_pt_min=args.trimmed_pt_min,
        trimmed_pt_max=args.trimmed_pt_max,
        trimmed_mass_min=args.trimmed_mass_min,
        trimmed_mass_max=args.trimmed_mass_max,
        shrink=args.shrink,
        shrink_mass=args.shrink_mass,
        compute_auxvars=True) 

    ievent = 0
    for event in clustering_generator:
        if event is None:
            continue
        dset_jet[ievent] = event.jets[0]
        dset_trimmed_jet[ievent] = event.jets[1]
        dset_subjets[ievent] = event.subjets
        dset_constit[ievent] = event.constit
        dset_trimmed_constit[ievent] = event.trimmed_constit
        dset_shrinkage[ievent] = event.shrinkage
        dset_dr_subjets[ievent] = event.subjet_dr
        dset_tau_1[ievent] = event.tau_1
        dset_tau_2[ievent] = event.tau_2
        dset_tau_3[ievent] = event.tau_3
        ievent += 1
        pbar.update(ievent)

    if ievent < num_events:
        # shrink tables:
        # the estimate of the number of events could have been too large
        # or some events didn't pass jet-level cuts
        dset_jet.resize((ievent,))
        dset_trimmed_jet.resize((ievent,))
        dset_subjets.resize((ievent,))
        dset_constit.resize((ievent,))
        dset_trimmed_constit.resize((ievent,))
        dset_shrinkage.resize((ievent,))
        dset_dr_subjets.resize((ievent,))
        dset_tau_1.resize((ievent,))
        dset_tau_2.resize((ievent,))
        dset_tau_3.resize((ievent,))

    pbar.finish()
    h5output.close()
    if not hepmc:
        h5input.close()


if __name__ == '__main__':

    import os
    from argparse import ArgumentParser

    parser = ArgumentParser()
    parser.add_argument('--jobs', '-n', type=int, default=-1)
    parser.add_argument('--jet-size', type=float, default=1.0)
    parser.add_argument('--subjet-size-fraction', type=float, default=0.5)
    parser.add_argument('--subjet-pt-min-fraction', type=float, default=0.05)
    parser.add_argument('--subjet-dr-min', type=float, default=0.)
    parser.add_argument('--shrink', default=False, action='store_true')
    parser.add_argument('--shrink-mass', default='wmass', choices=['jet', 'wmass'])
    parser.add_argument('--trimmed-mass-min', type=float, default=-1.)
    parser.add_argument('--trimmed-mass-max', type=float, default=-1.)
    parser.add_argument('--trimmed-pt-min', type=float, default=-1.)
    parser.add_argument('--trimmed-pt-max', type=float, default=-1.)
    parser.add_argument('--delphes', action='store_true', default=False)
    parser.add_argument('-d', '--debug', action='store_true', default=False,       
                        help="show stack trace in the event of "                
                            "an uncaught exception")
    parser.add_argument('files', nargs='+', help="HDF5 or HepMC file")
    args = parser.parse_args()

    if args.shrink_mass == 'jet':
        # shrink with jet mass
        args.shrink_mass = -1
    elif args.shrink_mass == 'wmass':
        args.shrink_mass = 80.385
    else:
        raise ValueError("invalid --shrink-mass option")

    from deepjets.parallel import map_pool, FuncWorker
    from deepjets.multi_progress import term
    from contextlib import contextmanager

    @contextmanager                                                                 
    def do_nothing(*args, **kwargs):                                                
        yield
    
    multi = len(args.files) > 1 and args.jobs != 1 
    context = term.fullscreen if multi else do_nothing
    
    with context():
        map_pool(
            FuncWorker,
            [(cluster_file, filename, args, multi, (0, i))
                for i, filename in enumerate(args.files)],
            n_jobs=args.jobs)
