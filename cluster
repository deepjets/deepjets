#!/usr/bin/env python
"""
Perform jet clustering on HepMC or HDF5 input
"""

def get_suffix(args):
    suffix = '_j{0:.1f}_sj{1:.2f}'.format(
        args.jet_size, args.subjet_size).replace('.', 'p')
    if args.delphes:
        suffix += '_delphes'
    if args.shrink:
        suffix += '_shrink'
    suffix += '_jets'
    return suffix


def cluster_file(filename, args, multi=False, location=None):
    import os
    import sys
    from deepjets.samples import create_jets_datasets
    from deepjets.generate import get_generator_input
    from deepjets.clustering import cluster
    from deepjets.detector import reconstruct
    from fnmatch import fnmatch
    from deepjets.multi_progress import Writer
    from progressbar import Bar, ETA, Percentage, FormatLabel, ProgressBar
    import numpy as np
    import h5py
    
    suffix = get_suffix(args)
    output = os.path.splitext(filename)[0] + suffix + '.h5'

    # check if output already exists and is finished
    if os.path.exists(output):
        try:
            h5py.File(output, 'r')
        except:
            print "{0} exists but is not complete. Will overwrite it.".format(output)
            pass
        else:
            print "{0} exists and is complete. Nothing to do.".format(output)
            return

    h5output = h5py.File(output, 'w')

    hepmc = fnmatch(os.path.splitext(filename)[1], '.hepmc*')
    if not hepmc:
        h5input = h5py.File(filename, 'r')

    if hepmc:
        cluster_input = get_generator_input('hepmc', filename)
        # assume we could be up to 20% wrong here
        num_events = long(cluster_input.estimate_num_events() * 1.2)
    else:
        cluster_input = h5input['events'] 
        num_events = len(cluster_input)

    if args.delphes:
        cluster_input = reconstruct(cluster_input)
    
    widgets = [FormatLabel(output), ' ', Percentage(), ' ', Bar('>'), ' ', ETA()]
    writer = Writer(location) if multi else sys.stderr
    pbar = ProgressBar(fd=writer, widgets=widgets, max_value=num_events).start()

    create_jets_datasets(h5output, num_events,
                         args.jet_size, args.subjet_size)

    dset_jet = h5output['jet']
    dset_trimmed_jet = h5output['trimmed_jet']
    dset_subjets = h5output['subjets']
    dset_constit = h5output['constituents']
    dset_trimmed_constit = h5output['trimmed_constituents']
    dset_shrinkage = h5output['shrinkage']
    dset_dr_subjets = h5output['subjet_dr']
    dset_tau_1 = h5output['tau_1']
    dset_tau_2 = h5output['tau_2']
    dset_tau_3 = h5output['tau_3']

    clustering_generator = cluster(
        cluster_input,
        jet_size=args.jet_size,
        subjet_size=args.subjet_size,
        subjet_pt_min_fraction=args.subjet_pt_min_fraction,
        subjet_dr_min=args.subjet_dr_min,
        trimmed_pt_min=args.trimmed_pt_min,
        trimmed_pt_max=args.trimmed_pt_max,
        trimmed_mass_min=args.trimmed_mass_min,
        trimmed_mass_max=args.trimmed_mass_max,
        shrink=args.shrink,
        shrink_mass=args.shrink_mass,
        compute_auxvars=True) 

    ievent = 0
    for event in clustering_generator:
        if event is None:
            continue
        dset_jet[ievent] = event.jets[0]
        dset_trimmed_jet[ievent] = event.jets[1]
        dset_subjets[ievent] = event.subjets
        dset_constit[ievent] = event.constit
        dset_trimmed_constit[ievent] = event.trimmed_constit
        dset_shrinkage[ievent] = event.shrinkage
        dset_dr_subjets[ievent] = event.subjet_dr
        dset_tau_1[ievent] = event.tau_1
        dset_tau_2[ievent] = event.tau_2
        dset_tau_3[ievent] = event.tau_3
        ievent += 1
        pbar.update(ievent)

    if ievent < num_events:
        # shrink tables:
        # the estimate of the number of events could have been too large
        # or some events didn't pass jet-level cuts
        dset_jet.resize((ievent,))
        dset_trimmed_jet.resize((ievent,))
        dset_subjets.resize((ievent,))
        dset_constit.resize((ievent,))
        dset_trimmed_constit.resize((ievent,))
        dset_shrinkage.resize((ievent,))
        dset_dr_subjets.resize((ievent,))
        dset_tau_1.resize((ievent,))
        dset_tau_2.resize((ievent,))
        dset_tau_3.resize((ievent,))

    pbar.finish()
    if not hepmc:
        if 'weights' in h5input:
            # copy weights
            h5input.copy('weights', h5output, name='generator_weights')
        h5input.close()
    h5output.close()


if __name__ == '__main__':

    from argparse import ArgumentParser

    parser = ArgumentParser()
    parser.add_argument('--jobs', '-n', type=int, default=-1)
    parser.add_argument('--jet-size', type=float, default=1.0)
    parser.add_argument('--subjet-size', type=float, default=0.5)
    parser.add_argument('--subjet-pt-min-fraction', type=float, default=0.05)
    parser.add_argument('--subjet-dr-min', type=float, default=0.)
    parser.add_argument('--shrink', default=False, action='store_true')
    parser.add_argument('--shrink-mass', default='wmass', choices=['jet', 'wmass'])
    parser.add_argument('--trimmed-mass-min', type=float, default=-1.)
    parser.add_argument('--trimmed-mass-max', type=float, default=-1.)
    parser.add_argument('--trimmed-pt-min', type=float, default=-1.)
    parser.add_argument('--trimmed-pt-max', type=float, default=-1.)
    parser.add_argument('--delphes', action='store_true', default=False)
    parser.add_argument('--batch', default=None)
    parser.add_argument('--dry', action='store_true', default=False)
    parser.add_argument('-d', '--debug', action='store_true', default=False,       
                        help="show stack trace in the event of "                
                            "an uncaught exception")
    parser.add_argument('files', nargs='+', help="HDF5 or HepMC file")
    args = parser.parse_args()

    import os
    import sys

    if args.batch is not None:
        # call me again but in a batch job for each input file
        import subprocess
        from deepjets.path_utils import mkdir_p

        cmd = sys.argv[:]
        # remove batch option
        idx = cmd.index('--batch')
        cmd.pop(idx)
        cmd.pop(idx)
        # remove filenames
        for filename in args.files:
            cmd.remove(filename)
        output_dir = os.getcwd()
        setup_cmd = "source {0}/setup.sh; cd {1};".format(
            os.path.dirname(os.path.realpath(__file__)),
            output_dir)
        log_path = os.path.join(output_dir, 'log')
        if args.dry:
            print "mkdir -p {0}".format(log_path)
        else:
            mkdir_p(log_path)
        # call self in batch job once per file
        for filename in args.files:
            name = os.path.splitext(os.path.basename(filename))[0] + get_suffix(args)
            cmd_file = ' '.join(cmd + [filename])
            cmd_batch = (
                'echo "{setup} {cmd_file}" | '
                'qsub -e {output_dir}/log -o {output_dir}/log '
                '-N {name} -l nodes=1:ppn=1 -q {queue};').format(
                    setup=setup_cmd,
                    cmd_file=cmd_file,
                    output_dir=output_dir,
                    queue=args.batch,
                    name=name)
            print cmd_batch
            if not args.dry:
                subprocess.call(cmd_batch, shell=True)
        sys.exit(0)

    if args.shrink_mass == 'jet':
        # shrink with jet mass
        args.shrink_mass = -1
    elif args.shrink_mass == 'wmass':
        args.shrink_mass = 80.385
    else:
        raise ValueError("invalid --shrink-mass option")

    from deepjets.parallel import map_pool, FuncWorker
    from deepjets.multi_progress import term

    if len(args.files) == 1:
        cluster_file(args.files[0], args)
    else:
        from contextlib import contextmanager
        
        @contextmanager                                                                 
        def do_nothing(*args, **kwargs):                                                
            yield 

        multi = len(args.files) > 1 and args.jobs != 1 and sys.stdout.isatty()
        context = term.fullscreen if multi else do_nothing

        with context():
            map_pool(
                FuncWorker,
                [(cluster_file, filename, args, multi, (0, i))
                    for i, filename in enumerate(args.files)],
                    n_jobs=args.jobs)
