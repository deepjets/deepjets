#!/usr/bin/env python


def train_one_point(model_name, files, learning_rate, batch_size):
    from deepjets.learning import cross_validate_model
    from deepjets.models import get_maxout
    import numpy as np
    import os
    model = get_maxout(25**2)
    batch_size = int(batch_size)
    # do not run CV in parallel
    vals = cross_validate_model(
        model, files,
        model_name=model_name + "_{0}_lr{1}_bs{2}".format(os.getpid(), learning_rate, batch_size),
        batch_size=batch_size, epochs=100,
        val_frac=0.1, patience=10,
        lr_init=learning_rate, lr_scale_factor=1.0,
        log_to_file=True, read_into_ram=True, max_jobs=1)
    return -1 * np.array(vals['AUC']).mean()


class ObjectiveFunction(object):
    def __init__(self, model_name, train_files):
        self.model_name = model_name
        self.train_files = train_files

    def __call__(self, args):
        from deepjets.parallel import map_gpu_pool, GPUWorker
        import numpy as np
        return np.array(map_gpu_pool(
            GPUWorker,
            [(train_one_point, self.model_name, self.train_files, learning_rate, batch_size)
                for learning_rate, batch_size in args],
            n_gpus=-1))


def bayesian_optimization(model_name, train_files):
    import GPyOpt
    from numpy.random import seed
    seed(12345)
    bounds = [(0.0001, 0.001), (32, 1024)]
    objective = ObjectiveFunction(model_name, train_files)
    bo = GPyOpt.methods.BayesianOptimization(f=objective, bounds=bounds)
    bo.run_optimization(max_iter=100, eps=10e-6)
    print bo.x_opt
    print bo.fx_opt


if __name__ == '__main__':

    from argparse import ArgumentParser

    parser = ArgumentParser()
    parser.add_argument('--sig', dest='signal_file')
    parser.add_argument('--bkg', dest='background_file')
    parser.add_argument('--name')
    parser.add_argument('--nfolds', type=int, default=5)
    parser.add_argument('--test-fraction', type=float, default=0.2)
    args = parser.parse_args()

    from deepjets.utils import prepare_datasets
    from deepjets.path_utils import mkdir_p
    
    mkdir_p('datasets')
    mkdir_p('models')

    dataset_name = 'datasets/' + args.name
    model_name = 'models/' + args.name
    
    # Prepare datasets once for all trainings
    h5_files = prepare_datasets(
        args.signal_file, args.background_file, dataset_name,
        test_frac=args.test_fraction, n_folds=args.nfolds,
        shuffle=True, balance=True)

    bayesian_optimization(model_name, h5_files['train'])
